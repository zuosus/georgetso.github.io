---
title: "Understanding MCP and the Agent-Led LLM World"
date: 2025-07-27
---

I've been hearing the term "MCP" (Model Context Protocol) floating around a lot lately, especially after my company's AI Week. I saw presentations and even a colleague demonstrating how simple it was to spin one up. I heard that MCP is easy to use, but I didn't have a good grasp of what it was.

After watching a short, three-part video series, I understood what an MCP is. I now see why it's needed and its role in the LLM ecosystem.

This led me to some discoveries. I learned about the "Agent" pattern and the "ReAct" (Reasoning and Acting) framework. This new knowledge cast a new light on a previous experience I had. I had been trying to use DeepSeek as a code generation model and noticed some platforms only offered "ask" or "edit" modes, with no "agent" capabilities. I had wondered if the model itself was the limitation. Now I realize that's likely not the case. It's more probable a strategic choice by the provider to steer users towards other models for agent-based tasks.

An analogy is: **If the LLM is the CPU, then the Agent is the conductor.** The LLM provides the raw processing power, but the Agent directs the symphony, coordinating tasks, and making decisions to achieve a complex goal.

The revelation around system prompts was a surprise. They can be very long and detailed for a sophisticated agent. When I think about the simple, ten-word system prompt I use for my calls on Cloudflare AI, the contrast is significant. It gave me a more realistic understanding of what "prompt engineering" truly entails. It's not just about asking a clever question; it's about architecting a detailed set of instructions and context to guide the model's behavior.

I have a new understanding of LLMs, agents, and prompts.